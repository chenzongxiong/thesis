\section{Gradient of hysteretic neural networks with respect to inputs}\label{sec:chapter3:gradient-networks}
First we only consider \textbf{one} \textit{play}
\begin{equation}\label{eqn:chapter3:outputs-of-pi-networks}
G(P_{n}, w^{1}) = \sum_{s=1}^{S} \tilde{\theta_{s}} a(\theta_{s} P_{n} + \theta_{s0}) + \tilde{\theta_{0}}
\end{equation}

where 
\begin{eqnarray}
    P_{n} &=& [p_{1}, p_{2}, \ldots, p_{n}] \\
    G(P_{n}, w^{1}) &=& [y_{1}, y_{2}, \ldots, y_{n}] \\
    \theta_{s} P_{n} &=& [\theta_{s} p_{1}, \theta_{s} p_{2}, \ldots, \theta_{s} p_{n}] \\
\end{eqnarray}
$\forall{s} \in [1, ..., S]$, and $a$ is an arbitrary activation function, such as $tanh, elu, relu, sigmoid$ function.

% $\theta_{i} P_{n} = (\theta_{i} p_{1}, \theta_{i} p_{2}, \ldots, \theta_{i} p_{n})$
% $P_{n} = [p_{1}, p_{2}, \ldots, p_{n}]$, $G(P_{n}, w^{1}) = [y_{1}, y_{2}, \ldots, y_{n}]$, $\forall{i} \in [1, ..., S]$, $\theta_{i} P_{n} = (\theta_{i} p_{1}, \theta_{i} p_{2}, \ldots, \theta_{i} p_{n})$,

% So 
% \begin{eqnarray}
%     a(\theta_{s} P_{n} + \theta_{s0}) = [a(\theta_{s} p_{1} + \theta_{s0}), a(\theta_{s} p_{2} + \theta_{s0}), \ldots, a(\theta_{s} p_{n} + \theta_{s0})]    
% \end{eqnarray}

% $\tanh(\theta_{i} P_{n} + \theta_{i0}) = [\tanh(\theta_{i} p_{1} + \theta_{i0}), \tanh(\theta_{i} p_{2} + \theta_{i0}), \ldots, \tanh(\theta_{i} p_{n} + \theta_{i0})]$

% So $\sum_{i=1}^{S} \tilde{\theta_{i}} \tanh(\theta_{i} P_{n} + \theta_{i0}) + \tilde{\theta_{0}} = [\sum_{i=1}^{S} \tilde{\theta_{i}} \tanh(\theta_{i} p_{1} + \theta_{i0}) + \tilde{\theta_{0}},
% \sum_{i=1}^{S} \tilde{\theta_{i}} \tanh(\theta_{i} p_{2} + \theta_{i0}) + \tilde{\theta_{0}},
% \ldots,
% \sum_{i=1}^{S} \tilde{\theta_{i}} \tanh(\theta_{i} p_{n} + \theta_{i0}) + \tilde{\theta_{0}}] =
% [y_{1}, y_{2}, ..., y_{n}]$.

Take $y_{j}$ for example, where $j \in [1, ..., n]$,
let $z_j=\theta_s p_j + \theta_{s0}$ and $a(z_j) = a(\theta_s p_j + \theta_{s0})$, we obtain
\begin{equation}\label{eqn:chapter3:TODO}
y_{j}  = \sum_{s=1}^{S} \tilde{\theta_{s}} a(\theta_{s} p_{j} + \theta_{s0}) + \tilde{\theta_{0}}  \\
      = \sum_{s=1}^{S} \tilde{\theta_{s}} a(z_j) + \tilde{\theta_{s0}}
\end{equation}

The derivative of $y_j$ with respect to $p_j$ is given by
% Calculate derivation for $y_{j}$,
\begin{equation}\label{eqn:chapter3:TODO}
\frac{\partial y_{j}}{\partial p_{j}} = \sum_{s=1}^{S} \tilde{\theta_{s}} \theta_{s} \frac{\partial a(z_j)}{\partial z_{j}} \frac{\partial z_{j}}{\partial p_{j}} 
\end{equation}

Now let's consider the mapping between $p_{j}$ and $x_{j}$ (see \myformularef{eqn:chapter2:phi}). Let $\sigma_{j} = w^{1} x_{j} - p_{j-1}$, the $p_j$ is given by 
\begin{equation}\label{eqn:chapter3:TODO}
p_{j} = \Phi(\sigma_{j}) + p_{j-1}
\end{equation}

% and

% \begin{equation}\label{eqn:chapter3:TODO}
% \Phi(x) =
%         \begin{cases}
%         x - 1/2, & x > 1/2 \\
%         0, & -1/2 < x < -1/2 \\
%         x + 1/2, & x < -1/2 \\
%         \end{cases}
% \end{equation}

Using chain rule, we obtain
\begin{equation}
\frac{\partial y_{j}}{\partial x_{j}} 
 =  \frac{\partial y_{j}}{\partial p_{j}} \frac{\partial p_{j}}{\partial x_{j}} \\
                                      = \sum_{s=1}^{S} \tilde{\theta_{s}} \theta_{s} w^{1} \frac{\partial a(z_j)}{\partial z_{j}} \frac{\partial{\Phi(\sigma_{j})}}{\partial{\sigma_{j}}}
\end{equation}


To consider \textbf{multiple} \textit{plays} case, we reformulate the derivation as following:

\begin{equation}
\frac{\partial {y_{j}^{1}}}{\partial x_{j}} = \frac{\partial{y_{j}^{1}}}{\partial{p_{j}^{1}}} \frac{\partial{ p_{j}}^{1}}{\partial x_{j}} \\
                                      = \sum_{s=1}^{S} \tilde{\theta_{s}^{1}} \theta_{s}^{1} w^{1} \frac{\partial a(z_{j}^{1})}{\partial z_{j}^{1}} \frac{\partial{\Phi(\sigma_{j}^{1})}}{\partial{\sigma_{j}^{1}}}
\end{equation}


As the architecture (see \myfigref{fig:chapter1:nn-arch}) shown, we know that if we have $K$ plays, the overall output is given by

% \begin{equation}
% F = \frac{1}{K} \sum_{k=1}^{K} G^{k}
% \end{equation}
% where $F=[f_1, f_2, ..., f_n]$,
% and
\begin{equation}
y_{j} = \frac{1}{P} \sum_{k=1}^{K} y_{j}^{k}
\end{equation}
So the derivation of $y_j$ with respect to $x_j$ is,
\begin{equation}
\frac{\partial y_{j}}{\partial x_{j}} = \frac{1}{K} \sum_{k=1}^{K} \frac{\partial {{y_{j}^{k}}}}{\partial {{x_{j}}}} \\
              = \frac{1}{K} \sum_{k=1}^{K} \frac{\partial {y_{j}^{k}}}{\partial {p_{j}^{k}}} \frac{\partial {p_{j}^{k}}}{\partial {x_{j}}} \\
              = \frac{1}{K} \sum_{k=1}^{K}  \sum_{s=1}^{S} \tilde{\theta_{s}^{k}} \theta_{s}^{k} w^{k} \frac{\partial a(z_{j}^{k})}{\partial z_{j}^{k}} \frac{\partial{\Phi(\sigma_{j}^{k})}}{\partial{\sigma_{j}^{k}}}
\end{equation}


