\chapter{Evaluation\label{cha:chapter5}}

% \section{Attribute to compare}\label{}
% \mytodo{complexity of synthesis data set (nb plays/nb units/random weights/noise)} 
% \mytodo{number of sequence length (points)} 
% \mytodo{number of parameters to use in the network, our network is smaller and can  achieve the same performance as (lstm/rnn/gru)} 
% \mytodo{training loss curve} 
% \mytodo{time consumption during train/prediction} 
% \mytodo{(mean square error/cross entropy) on prediction data set}
% In this chapter, we evaluate different methods on hysteretic data sets and compare the performance among them.

\section{Data sets}
We generate data sets to identify whether LSTM/SimpleRNN/GRU network and HNN could capture the micro-loops of a hysteretic process. 
\newline
\textbf{Data set $D$.} The set $D$ (see \myfigref{fig:chapter5:dima-seq} and \myfigref{fig:chapter5:dima-seq-2}) consists 1000 points. We repeat sequence $0, 3, 5, 0, 1, 5$ to generate the input $x(n)$ for $D$. We also insert $0, -100$ at the very beginning of the input $x(n)$ to erase the memory in the hysteretic process. The first 600 points are used as a training set, and the rest 400 points are for the test set. We rescale the input of test set by 1/10, 1/7, 1/6, 1/5, 1/4, 1/3, 1/1, 1/0.5 every 50 points to identify inner loops. In this artificial set, only customized hysteretic loops are exposed to networks throughout the training phase.
% The micro and macro loops in test set  any model during learning. 
\newline
\textbf{Data set $P$.} For the set $P$ (see \myfigref{fig:chapter5:pavel-seq} and \myfigref{fig:chapter5:pavel-seq-2}), it also contains 1000 points. The inputs $x(n)$ is sampled from a periodical function $5cos(0.1n)$ with random noise $noise$. Repeatedly, $noise$ is drawn from different normal distributions, which standard deviations are $0.1, 0.5, 1, 2, 3, 4, 5$. \todo{why do I use this data set}
\newline
\textbf{Data set $S$.} We generate set $S$ (see \myfigref{fig:chapter5:sin-seq} and \myfigref{fig:chapter5:sin-seq-2}) with different number of points. The inputs $x(n)$ of $S$ is sampled from a periodical signal function $\cos(0.1n) + 0.3\sin(1.3n) + 1.2\sin(0.6n)$. 
\newline
% \textbf{Data set $M$} \mytodo{the outputs is a random walk}

% \begin{figure}[h!]
%     \centering
%     \subfloat[]{
%     \includegraphics[height=7cm,width=\textwidth]{thesis/img/debug-input-output-dima.pdf}
%     }
%     \caption{Data set D. The first and second plots are inputs vs. time step $n$ and outputs vs. time step $n$, respectively. Time step $n$ is between 10 and 1000. The last one is outputs vs. inputs, only showing $y(n) \in [-0.5, 45]$}
%     \label{fig:chapter5:dima-seq}
% \end{figure}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[height=7cm,width=\textwidth]{thesis/img/debug-input-output-pavel.pdf}
%     \caption{Data set P. The first and second plots are inputs vs. time step $n$ and outputs vs. time step $n$, respectively. Time step $n$ is from 0 to 1000. The last one is outputs vs. inputs, only showing $y(n) \in [-0.5, 45]$}
%     \label{fig:chapter5:pavel-seq}
% \end{figure}

% \begin{figure}[h!]
%     \centering
%     \subfloat[]{
%         \includegraphics[height=5cm,width=\textwidth]{thesis/img/debug-input-output-sin-2.pdf}
%     }
%     \hfill
%     \subfloat[]{
%         \includegraphics[height=5cm,width=10cm]{thesis/img/debug-input-output-sin.pdf}
%     }
%     \caption{Data set S. The first and second plots are inputs vs. time step $n$ and outputs vs. time step $n$, respectively. Time step $n$ is from 0 to 1000.}
%     \label{fig:chapter5:s-seq}
% \end{figure}
\begin{figure}[ht!]
    \centering

    \subfloat[]{
        \includegraphics[width=\textwidth/3]{debug-input-output-dima.pdf}
                 \label{fig:chapter5:dima-seq-2}

    }
    \subfloat[]{
        \includegraphics[width=\textwidth/3]{debug-input-output-pavel.pdf}
                         \label{fig:chapter5:pavel-seq-2}

    }
    \subfloat[]{
        \includegraphics[width=\textwidth/3]{debug-input-output-sin.pdf}
                         \label{fig:chapter5:sin-seq-2}

    }

    \hfill
    
    \caption{Caption}
    \label{fig:chapter5:data-sets}
\end{figure}

\begin{figure}[ht!]
    \centering
    \subfloat[]{
        \includegraphics[height=3cm,width=\textwidth]{debug-input-output-dima-2.pdf}
         \label{fig:chapter5:dima-seq}
    }
    
    \hfill
    \subfloat[]{
        \includegraphics[height=3cm,width=\textwidth]{debug-input-output-pavel-2.pdf}
                 \label{fig:chapter5:pavel-seq}

    }
    \hfill
    \subfloat[]{
        \includegraphics[height=3cm,width=\textwidth]{debug-input-output-sin-2.pdf}
                 \label{fig:chapter5:sin-seq}

    }
    \caption{Data sets.}
    \label{fig:chapter5:data-sets}
\end{figure}
\FloatBarrier

\section{Architectures}\label{sec:chapter5:architectures}

\textbf{Hyperparameters}
In this evaluation, we focus on optimizing the following hyper parameters in LSTM/SimpleRNN/GRU networks and HNN.
\begin{table}[h!]
\begin{center}
    \begin{tabular}{||c|c||}
    \hline
    Name                  & Explanation \\
    \hline 
    \#nb\_plays           & the number of $plays$ in ground-truth data sets \\
    \hline
    \#units               & the number of $units$ in ground-truth data sets \\
    \hline
     \#\_\_nb\_plays\_\_  & the number of $plays$ used in training phase \\
     \hline
     \#\_\_units\_\_      & the number of $units$ used in training phase \\
    \hline
    \end{tabular}
    \caption{Hyperparameters used for evaluation. \#\_\_nb\_plays\_\_ and \#\_\_units\_\_  can be different from the ground-truth parameters since they're blind to us in reality.}
    \label{tbl:chapter5:hyperparameters}    
\end{center}
\end{table}

\textbf{LSTM network.} We use one LSTM layer with tanh nonlinearities and one dense layer. We use the default settings of hyper parameters in tensorflow \citep{abadi2016tensorflow} except for the hidden units in LSTM layer. We grid search a sequence of hidden units, $1, 8, 16, 32, 64, 128, 256$, to find out the best results achieved by LSTM networks. As for the loss functions, we minimize mean squared error (MSE) and minimize the negative maximal likelihood estimator (MLE). The total number of parameters used in this architecture is given by
\begin{equation*}
    \text{\#parameters} = 4 * ({\text{\#\_\_units\_\_}}^2 + 2*\text{\#\_\_units\_\_}) + \text{\#\_\_units\_\_} + 1
\end{equation*}
\textbf{SimpleRNN/GRU network.} As for SimpleRNN/GRU network, we use the same architecture as LSTM network except the recurrent block. In SimpleRNN/GRU network, we apply SimpleRNN/GRU block instead of LSTM block. The total number of parameters used in these two different networks are given by 

\begin{equation*}
   \text{\#parameters} = ({\text{\#\_\_units\_\_}}^2 + 2*\text{\#\_\_units\_\_}) + \text{\#\_\_units\_\_} + 1    
\end{equation*}
and
\begin{equation*}
   \text{\#parameters} = 3 * ({\text{\#\_\_units\_\_}}^2 + 2*\text{\#\_\_units\_\_}) + \text{\#\_\_units\_\_} + 1    
% \text{\#parameters} = 3 * \text{\#\_\_units\_\_} + 3 * \text{\#\_\_units\_\_} * \text{\#\_\_units\_\_} + 3 * \text{\#\_\_units\_\_} + \text{\#\_\_units\_\_} + 1
\end{equation*}
 respectively.


% where $\#units$ is the number of hidden units.

\textbf{HNN.}  We use architecture shown in \mychapterref{cha:chapter1} (see \myfigref{fig:chapter1:nn-arch}). We also exploit hyperparameters $\_\_plays\_\_$, $\_\_units\_\_$ in HNN. The number of parameters $\#parameters$ used in HNN is given by
\begin{equation*}
    \text{\#parameters} = 3 * \text{\#\_\_plays\_\_} * \text{\#\_\_units\_\_} + 1
\end{equation*}
% where $\#play$ is the number of plays and $\#units$ is the number of units.


\section{Measure}
The overall \textit{root mean squared error} (RMSE) is used to measure the quality of fit. It's defined as following,
% \begin{equation}
% \text{RMSE} = \sqrt{\frac{\sum_{i=0}^{N-1}(\hat{y}_i - y_i)^2}{N}}
% \end{equation}
% Additionally, we defined RMSE(n) as
\begin{equation}
    \text{RMSE(n)} = \sqrt{\frac{\sum_{i=n}^{N-1}(\hat{y}_i - y_i)^2}{N-n}}, \quad n=0,1,\ldots,N-1
\end{equation}
where $n$ means the first $(n-1)$th data points are ignored in RMSE results.

Particularly, we set $\text{RMSE}=\text{RMSE(0)}$ when $n=0$.

% \mytodo{For MLE, we use alternative criteria to measure the result of mu and sigma.}
\input{thesis/chapters/chapter5/micro-experiments}
\input{thesis/chapters/chapter5/debug-mse}
% \newpage
% \input{thesis/chapters/chapter5/debug-mle}

% \mytodo{same weights but different nb plays}
% \mytodo{different weights and different plays  no noise}
% \mytodo{different weights  and different plays noise} 
% \input{thesis/chapters/chapter5/same-weights-no-noise}
% \input{thesis/chapters/chapter5/different-weights-no-noise}
% \input{thesis/chapters/chapter5/different-weights-noise}
\newpage
\section{HNN for market model}\label{sec:chapter5:hnn-for-market-model}
In this section, we only compare HNN with LSTM network since SimpleRNN and GRU don't perform much better than LSTM.
\input{thesis/chapters/chapter5/synthetic-stock-dataset}
\input{thesis/chapters/chapter5/predicting-price}
% \input{thesis/chapters/chapter5/real-word-dataset}

% \mytodo{add real world data set}

% We scale the the results by $\frac{x-\mu}{\sigma}$

% \mytodo{HERE is the sigma is 50, we will try sigma is 10 throughout this paper}
