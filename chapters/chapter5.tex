\chapter{Evaluation\label{cha:chapter5}}

% \section{Attribute to compare}\label{}
% \mytodo{complexity of synthesis data set (nb plays/nb units/random weights/noise)} 
% \mytodo{number of sequence length (points)} 
% \mytodo{number of parameters to use in the network, our network is smaller and can  achieve the same performance as (lstm/rnn/gru)} 
% \mytodo{training loss curve} 
% \mytodo{time consumption during train/prediction} 
% \mytodo{(mean square error/cross entropy) on prediction data set}
In this chapter, we evaluate different methods on hysteretical data sets and compare the performance among them.

\section{Data sets}
We generate data sets to identify whether LSTM network and HNN could capture the micro loops of a hysteretical process. 
\newline
\textbf{Data set $D$}. The set $D$ (see \myfigref{fig:chapter5:dima-seq}) consists 1000 points. We repeat sequence $0, 3, 5, 0, 1, 5$ to generate the input $x(n)$ for $D$. We also insert $0, -100$ at the very beginning of the input $x(n)$ in order to erase the memory in the hysteretical process. The first 600 points is used as training set and the rest 400 points is test set. We scale the input of test set by 1/10, 1/7, 1/6, 1/5, 1/4, 1/3, 1/1, 1/0.5 every 50 points in order to identify inner loops.  
\newline
\textbf{Data set $P$} As for the set $P$ (see \myfigref{fig:chapter5:pavel-seq}), it also contains 1000 points. The inputs $x(n)$ is sampled from a periodical function $5cos(0.1n)$ with random noise $noise$. Repeatedly, $noise$ is drawn from different normal distributions, which standard deviations are $0.1, 0.5, 1, 2, 3, 4, 5$. 
\newline
\textbf{Data set $S$}. We generate set $S$ (see \myfigref{fig:chapter5:s-seq}) with different number of points. The inputs $x(n)$ of $S$ is sampled from a periodical signal function $\cos(0.1n) + 0.3\sin(1.3n) + 1.2\sin(0.6n)$. 
\newline
\textbf{Data set $M$} \mytodo{the outputs is a random walk}

% \begin{figure}[h!]
%     \centering
%     \subfloat[]{
%     \includegraphics[height=7cm,width=\textwidth]{thesis/img/debug-input-output-dima.pdf}
%     }
%     \caption{Data set D. The first and second plots are inputs vs. time step $n$ and outputs vs. time step $n$, respectively. Time step $n$ is between 10 and 1000. The last one is outputs vs. inputs, only showing $y(n) \in [-0.5, 45]$}
%     \label{fig:chapter5:dima-seq}
% \end{figure}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[height=7cm,width=\textwidth]{thesis/img/debug-input-output-pavel.pdf}
%     \caption{Data set P. The first and second plots are inputs vs. time step $n$ and outputs vs. time step $n$, respectively. Time step $n$ is from 0 to 1000. The last one is outputs vs. inputs, only showing $y(n) \in [-0.5, 45]$}
%     \label{fig:chapter5:pavel-seq}
% \end{figure}

% \begin{figure}[h!]
%     \centering
%     \subfloat[]{
%         \includegraphics[height=5cm,width=\textwidth]{thesis/img/debug-input-output-sin-2.pdf}
%     }
%     \hfill
%     \subfloat[]{
%         \includegraphics[height=5cm,width=10cm]{thesis/img/debug-input-output-sin.pdf}
%     }
%     \caption{Data set S. The first and second plots are inputs vs. time step $n$ and outputs vs. time step $n$, respectively. Time step $n$ is from 0 to 1000.}
%     \label{fig:chapter5:s-seq}
% \end{figure}

\begin{figure}
    \centering
    \subfloat[]{
        \includegraphics[height=3cm,width=\textwidth]{debug-input-output-dima-2.pdf}
    }
    \hfill
    \subfloat[]{
        \includegraphics[height=3cm,width=\textwidth]{debug-input-output-pavel-2.pdf}
    }
    \hfill
    \subfloat[]{
        \includegraphics[height=3cm,width=\textwidth]{debug-input-output-sin-2.pdf}
    }
    \hfill
    \subfloat[]{
        \includegraphics[width=\textwidth/3]{debug-input-output-dima.pdf}
    }
    \subfloat[]{
        \includegraphics[width=\textwidth/3]{debug-input-output-pavel.pdf}
    }
    \subfloat[]{
        \includegraphics[width=\textwidth/3]{debug-input-output-sin.pdf}
    }
    \caption{Data sets.}
    \label{fig:chapter5:data-sets}
\end{figure}

\section{Architectures}
\textbf{LSTM network.} We use one LSTM layer with tanh nonlinearities and one dense layer. We use the default settings of hyper parameters in tensorflow \citep{abadi2016tensorflow} except the hidden units $units$ in LSTM layer. We grid search a sequence of hidden units, $1, 8, 16, 32, 64, 128, 256$, to find out the best results achieved by LSTM networks. As for the loss functions, we minimize mean squared error (MSE) and minimize negative maximal likelihood estimator (MLE). The total number of parameters used in this architecture is given by
\begin{equation*}
    \text{\#parameters} = 4 * ({\text{\#\_\_units\_\_}}^2 + 2*\text{\#\_\_units\_\_}) + \text{\#\_\_units\_\_} + 1
\end{equation*}
% where $\#units$ is the number of hidden units.

\textbf{HNN.}  We use architecture shown in \mychapterref{cha:chapter1}, see \myfigref{fig:chapter1:nn-arch}. We also exploit hyper parameters $plays$. $units$ in HNN. The number of parameters $\#parameters$ used in HNN is given by
\begin{equation*}
    \text{\#parameters} = 3 * \text{\#\_\_plays\_\_} * \text{\#\_\_units\_\_} + 1
\end{equation*}
% where $\#play$ is the number of plays and $\#units$ is the number of units.

\textbf{Hyper parameters}
In this evaluation, we focus on optimizing the following hyper parameters in LSTM networks and HNN.
\begin{table}[h!]
\begin{center}
    \begin{tabular}{||c|c||}
    \hline
    Name                  & Explanation \\
    \hline 
    \#nb\_plays           & the number of $plays$ in ground-truth data sets \\
    \hline
    \#units               & the number of $units$ in ground-truth data sets \\
    \hline
     \#\_\_nb\_plays\_\_  & the number of $plays$ used in training phase \\
     \hline
     \#\_\_units\_\_      & the number of $units$ used in training phase \\
    \hline
    \end{tabular}
    \caption{Hyper parameters used for evaluation}
\end{center}
\end{table}

\section{Measure}
The overall \textit{root mean squared error} (RMSE) is used to measure the quality of fit if loss function is MSE. It's defined as following,
% \begin{equation}
% \text{RMSE} = \sqrt{\frac{\sum_{i=0}^{N-1}(\hat{y}_i - y_i)^2}{N}}
% \end{equation}
% Additionally, we defined RMSE(n) as
\begin{equation}
    \text{RMSE(n)} = \sqrt{\frac{\sum_{i=n}^{N-1}(\hat{y}_i - y_i)^2}{N-n}}, \quad n=0,1,\ldots,N-1
\end{equation}
where $n$ means the first $(n-1)$th data points are ignored in RMSE results.

Especially, we set $\text{RMSE}=\text{RMSE(0)}$ when $n=0$.

\mytodo{For MLE, we use alternative criteria to measure the result of mu and sigma.}
\input{thesis/chapters/chapter5/micro-experiments}
\input{thesis/chapters/chapter5/debug-mse}
\input{thesis/chapters/chapter5/debug-mle}

% \mytodo{same weights but different nb plays}
% \mytodo{different weights and different plays  no noise}
% \mytodo{different weights  and different plays noise} 
% \input{thesis/chapters/chapter5/same-weights-no-noise}
% \input{thesis/chapters/chapter5/different-weights-no-noise}
% \input{thesis/chapters/chapter5/different-weights-noise}
\section{HNN for market model}\label{}
% \mytodo{hnn for markov chain dataset}
% \mytodo{compare network complexity, mle loss} 
% \mytodo{compare the markov chain obtain by lstm and hnn}
% \mytodo{show animation that lstm performs worse due to cannot capture minor loops.}
\mytodo{show that hnn can reconstruct agents behavior}
\mytodo{show that hnn can give a distribution of price, not average of price} 
\mytodo{show that hnn can explain the avalanche of market} 
\mytodo{add real world dataset}

We scale the the results by $\frac{x-\mu}{\sigma}$

\mytodo{HERE is the sigma is 50, we will try sigma is 20 throughout this paper}
% \section{Synthetic data sets}\label{sec:chapter3:synthetic-datasets}
% Following the practical mentioned in previous section, we obtain the following artificial data sets. See \myfigref{fig:chapter3:agents-distribution,fig:chapter3:market-participanted-agents,fig:chapter3:market-ground-truth-dataset}
% % \mytodo{add distribution of real agent D, virtual agent N}

% \begin{figure}[htb!]
%     % \left
%     %\subfloat[]{
%         %\raisebox{0px} {
%             % \adjustimage{width=\textwidth/2}{
%             \includegraphics[height=3cm,width=\textwidth/2]{market-real-agents-n-distribution}\label{fig:chapter3:market-real-agents-n-distribution}
%             \includegraphics[height=3cm,width=\textwidth/2]{market-virtual-agents-d-distribution}\label{fig:chapter3:market-virtual-agents-d-distribution}
             
%             % \adjustimage{width=\textwidth/2}{market-virtual-agents-d-distribution}\label{fig:chapter3:market-virutal-agents-d-distribution}

%         % }
%     % }

%     % \subfloat[]{
%     %         \includegraphics[width=\textwidth/2]{market-virtual-agents-d-distribution}
%     %                                 \label{fig:chapter3:market-virutal-agents-d-distribution}

%     % }
%     % \hfill
%     %     \subfloat[]{
%              % \adjustimage{width=\textwidth/2,left}{market-real-agents-alpha-distribution}
%              % \label{fig:chapter3:market-real-agents-alpha-distribution}
%             \includegraphics[height=3cm,width=\textwidth/2]{market-real-agents-alpha-distribution}\label{fig:chapter3:market-real-agents-alpha-distribution}

%     % }
%     \caption{\myfigref{fig:chapter3:market-real-agents-n-distribution} shows the distribution of real agents N. \myfigref{fig:chapter3:market-real-agents-alpha-distribution} shows the distribution of $\alpha$ for each agent. \myfigref{fig:chapter3:market-virutal-agents-d-distribution} shows the distribution of virtual agents D}
%     \label{fig:chapter3:agents-distribution}
% \end{figure}

% \begin{figure}[t!]
%     \centering
%     \subfloat[]{
%     \includegraphics[height=7cm,width=\textwidth]{market-participanted-agents}
%     }
%     \caption{The detailed behaviours of agents taken part in the market during simulation}
%     \label{fig:chapter3:market-participanted-agents}
% \end{figure}

% \begin{figure}[t!]
%     \centering
%     \subfloat[]{
%         \includegraphics[height=7cm,width=\textwidth]{market-ground-truth-dataset}
%     }
%     \caption{The synthetic data sets generated from the model. The top one is the random walk, which is underline in the real market, of the simulation. The middle one is the fluctuation of price based on the model. And the bottom one the change of the total number of stocks in the market, which is following to random walk.}
%     \label{fig:chapter3:market-ground-truth-dataset}
% \end{figure}