Training methods
There are some commonly used techniques when training RNN model:
Stochastic optimization:Adaptive moment estimation (Adam) was presented by Diederik Kingma [31] in 2015 which consists both advantages from AdaGrad and RMSProp. It
can update alpha weight during the learning process. It needs less memory and calcula- tion efficiently.
Gradient clipping: Goodfellow [2] indicates the cliffs commonly occur in RNN. By limiting the magnitude of gradient, it can perform better in the vicinity of cliffs.
Regularization:Dropout is ineffective when applied to recurrent connections, as re- peated random masks zero all hidden units in the limit. The most common solution is only to apply dropout to non-recurrent connections. Gal et al[4] proposed a variational inference based dropout method. From the Bayesian view, the RNN model treated as probability model and using same dropout mask at each time step for both inputs, out- puts, and recurrent layers. Actually, it uses recurrent dropout via dropping weights.
L2 regularization: L2 is the sum of the square of the weights which used to pre- vent the coefficients to fit so perfectly to overfitting. λnorm is a method to compute the L2 norm of a vector or tensor.
Early stopping: It is a regularization method to avoid overfitting when training a model with an iterative method such as gradient descent.

L2 regularization: L2 is the sum of the square of the weights which used to pre- vent the coefficients to fit so perfectly to overfitting. λnorm is a method to compute the L2 norm of a vector or tensor.
Early stopping: It is a regularization method to avoid overfitting when training a model with an iterative method such as gradient descent.
Cell initialization: The earliest deep learning began with repeated matrix multi- plications. The most important feature of the orthogonal matrix for us is that all the eigenvalues of the orthogonal matrix have an absolute value of 1. This means that no matter how many repeated matrix multiplications are performed, the resulting matrix will not explode or disappear.
LSTM forget bias: Rafal [32]recommended using 1.0 as forget gate bias to retain more information.
Learning rate: [32] also states When the learning rate is low, then training is more reliable, but optimization will take much time because steps towards the minimum of the loss function are tiny. When the learning rate is high, then training may not converge or even diverge. Weight changes can be so big that the optimizer overshoots the minimum and makes the loss worse. It is better to start from a relatively large learning rate like 0.1, then try exponentially lower values like 0.01, 0.001, etc.