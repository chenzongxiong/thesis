% Since 2013, Institute of Electrical and Electronics Engineers Audio and Acoustic Sig- nal Process (IEEE AASP) held the competition about Detection and Classification of Acoustic Scenes and Events (DCASE), aims to evaluate existing acoustic scenes de- tection methods. Recent submissions to the challenges (Sound event detection in real life audio, DCASE 2017) have suggested that the classical methods are being replaced by deep learning was not accidental, that they are different representations of different computing power on the same issue. The foundation of the theory of neural networks today originated in 1890, the psychologist William James published a monograph on the structure and function of human brain “Principles of Psychology.” He believes that a nerve cell can be stimulated to transmit to another nerve cell after being stimulated, and the activation of nerve cells is the result of all the input of the cell. The development of artificial neural networks experienced three periods: cybernetics from the 1940s to the 1960s, connectionism from the 1980s to the mid-1990s, and deep learning since 2006.
% In 1943, the neuroscientist and control expert McCulloch and logician Pitts presented the mathematical description and structure of neurons. And it turns out that as long as there are enough simple neurons, any calculation function (M-P model) can be sim- ulated if these neurons are interconnected and run synchronously. In the late 1940s, psychologist Donald Hebb proposed Hebbian learning based on the mechanism of neural
% plasticity, which is considered as a typical unsupervised learning rule. In 1957, American psychologist Frank Rosenblatt proposes a neural network with a single-layer computing unit called the Perceptron, inspired by biological neural networks that process informa- tion from the human brain[20]. It is a simple two-layer associative networks in which the input pattern arriving at input layer are mapped directly to output layer. Such two layers network only contains input and output units but not hidden unit. Therefore, there is no internal representation between the input and output mapping and it only capable of learning linear problems. Standard multi-layer perceptron (MLP) is known as a neural network that has unlimited number of neurons in each hidden layer to approx- imate an arbitrary continuous (nonlinear) function to any desired degree of accuracy. At this moment, there is no way to train the hidden units. In 1969, Minisky and Pa- pert discuss the bottleneck of the MLP and pointed out that theoretically it cannot be proved that it is meaningful to extend the perceptron model to a multi-layer network. Because for the nodes of each hidden layer, they do not have expected output, so it is impossible to train multi-layer perceptrons through learning rules. Consequently, re- search on artificial neural networks had stagnated, and other linear machine learning methods such as Support Vector Machine (SVM) was gradually exceeds that of neu- ral networks. But the study of shallow networks and related theories has accumulated a good theoretical and empirical foundation for the further development of deep learning.
% In 1974, Dr. Paul Werbos first proposed using Error Back Propagation (BP) which is essentially a chain rule of complex functions. In 1986, Rumelhart and Hinton used BP effectively trained some shallow networks[22], which solved the challenge of Minisky and Papert and rekindle the hope of neural networks. Its basic idea is that the learning pro- cess consists of two processes: positive propagation of signals and backward propagation of errors. BP aims to find a powerful synaptic weight adjustments rule to develop an internal structure that is appropriate for a particular task domain. Specifically, the hid- den state should be capable of learning when being active to simulate the input-output pattern. It may be trapped in local minima when looking for the optimum value of gradient descent. In 1989, the MLP with as few as one hidden layer had been proved in [21] is indeed capable of universal approximation in a satisfactory scene. Hanki and Stinchcombe established a mapping function based on this MLP which implies any un- successful mapping must arise from insufficient learning, numbers of hidden units or the lack of a deterministic relationship between input and target.
% Since 2000, with the rapid development of the Internet, although shallow learning net- works have achieved great success, the disadvantages of shallow networks have become more obvious. For example, the independence assumptions of Hidden Markov Models (HMMs) is unrealistic and their representational capacity is limited by hidden states. Until 2006, deep learning was indeed applied to many pattern recognition fields such as speech processing and image recognition. Hinton’s research [10] shows that it is entirely feasible to train a deep densely connected deep network such as Depp Belief Network. In the fine-tuning process, the BP algorithm is used to find the optimum global value as
% accurately as possible over a good initial value range. The deep learning model is used by many scholars in the field of speech recognition because of its good representation of features and the accuracy of solving complex speech problems. Mohamed et al. [23] used DBN to identify phonemes on the TIMIT database and achieved a 23\% error rate. It also include the exploration of effect of layer size in which the performance starts to plateau when adding more layers than three. Dahl et al. [24] proposed a method for constructing a new acoustic model combining pre-trained DNNs and HMMs, replacing the traditional GMM-HMM model, achieving an accuracy of 71.8\%. However, almost all conditional generative models face a problem. If the current model’s prediction is based only on the most recent inputs, and the inputs themselves are predicted by the model, then it is highly unlikely that it will recover from past mistakes. Having a longer memory has a stable effect because even if the network misunderstand its recent history, it can also review earlier rules. Consequently, saving longer-term memories is a more profound and effective solution.

% Recurrent neural networks (RNNs) is a kind of neural networks (Rumelhart et al. 1986), which is dedicated to mapping sequential data, such as text, genomes, handwriting, the spoken word, or numerical times series data emanating from sensors and stock markets. It is a dynamic model that can be trained for time series generation by processing se- quence one step at a time and predicting what the next most likely is. Unlike other neural networks, RNNs use their internal representation to perform a high dimensional interpolation between consecutive samples. It implements a mechanism similar to that of the human brain that retains a certain memory of processed information, unlike other types of neural networks that do not retain the memory of processed information. Good- fellow et al. [2] explain RNNs originated from an idea in machine learning and statistical models of 1980s: sharing parameters across different parts of a model, which makes it possible to extend the model to examples of different lengths of a sequence. It is a deep version of time-delay neural network which use convolution filter span over 1-D temporal sequence. It is a shallow representation than RNN since the output of convolution filter is an operation of a small pieces of neighbouring members of the input. The critical component of RNNs is their recurrent connections that it can store previous information and use it as part of the input for next step. This structure allows it to keep previ- ous information through a very deep computational graph, but it also brings a problem when the interval of the dependency to be captured become longer. This is likely due to the insufficient use of these potentially compelling and suitable time-series models. Bengio [25] claimed when the time span of correlation increases, the highly unstable relationship between the parameters and the hidden layer causes the gradient descent fails to train the RNN properly. It is called vanishing or exploding [26] gradients where the weights and activation function affects the magnitude of the gradient to blows up or decays exponentially as it cycles around the recurrent connections. It is also related to a trade-off for long term or short term cell states.
% Before Hochreiter and Schmidhuber [27] proposed a modified architecture call the Long
% Short-term Memory (LSTM), the latest research results for the RNN model was often the same as random guessing. At the meantime, there were two main ways of incor- porating previous information into sequence processing tasks: decompose inputs into overlapping time-windows and treat it as spatial, or use recurrent connections to build the relation directly. However, they both have drawbacks: time-windows size is highly dependent on task requirement, too small will underfitting or too large will overfitting; standard RNNs is known to have difficulty learning long-term information. The stan- dard LSTM constructed by an RNN architecture with an internal memory cell which specializes in transmitting long-term information, along with four gates which allow the memory cell to store and control information flow over long periods of time. The internal memory cell is designed to have fixed linear dynamics with a self-connection of value one; then the gradient will not vanish or explode as it is back propagated through them. LSTM has been given state-of-the-art results in a variety of sequential problem. For example, Graves [28] successfully applied bidirectional LSTM to speech recognition and outperformed unidirectional LSTM which suggests both directions are equally impor- tant. LSTM is efficient to converge and also more accurate than both standard RNNs and time-windowed MLPs. Besides, they found out LSTM benefits more from longer target delays than RNNs since LSTM can make use of the extra context without suf- fering as much from the expense of maintaining previous inputs. Based on standard LSTM, the stacked LSTM [29] [33] is a deeper RNNs composed of multiple LSTM layers on top of each other which inspired by standard RNNs benefits from depth in space. This approach potentially allows each level of hidden states to operate information at a different timescale.
% For deep RNNs, although increase the size of the recurrent layer increases memory capacity with a quadratic slow down, deepening networks in multiple dimensions can improve their representational efficiency and memory capacity with a linear complex- ity cost. However, all these architecture above only add RNN or LSTM cell along the temporal dimension. Since RNN can only be applied in one dimension in the past, it means that those 2D or multidimensional problems need to be compressed to 1D in advance, such as image recognition need to be processed line by line. It is intuitively to make the network have access to their surrounding context in all directions. Therefore, [30] proposed multi-dimensional RNN (MDRNNs) which adds hidden to hidden connec- tion to every dimension of data. It is an extension of the bidirectional recurrent neural network (BRNNs) that includes two hidden layers connected to a single output layer. Therefore, it has access to both past and future context. Similar to BRNNs, MDRNNs contains 2n separate hidden layers. In the feed-forward pass, each hidden layer at the network receives specific input and its activation from one time step back along with all dimensions of data. One-dimensional LSTM can be applied in MDRNNs by using multiple memory which corresponds to each dimension in the data with individual forget gate. Although the LSTM cell in MDRNNs receives multiple inputs and memory cells, it only produces one memory cell and one hidden state by linear accumulation which brings some numerical problems. As the number of the path in the grid grows with the combination of the size of each dimension, the accumulation of memory cell will be unstable.
% MDRNNs has multiple memory cells but not distinct for each dimension so that only ap- plied to the temporal dimension. Various neural network architecture has been proposed in past five years to model correlations of the input signal in both frequency and time dimensions. For example, convolutional neural networks (CNNs), Frequency LSTMs [35], Time-Frequency LSTMs [30] and grid LSTMs [10]. Frequency and Time-Frequency LSTMs are alternatives to CNNs to model correlations in frequency which are more adaptive and robust to the input signal with noise conditions. Google Deep Mind [10] proposed a simple alternative that can solve the computational instability in MDRNNs by producing distinct memory cells for each dimension in the grid. Grid Long Short-term Memory (Gird-LSTM) is a network of LSTM cells can expand depth in any dimension of the network. One dimensional Grid-LSTM is very close to high way neural network where it is a feed-forward neural network with LSTM cell instead of transferring function such as tanh or relu. Two-dimensional Grid-LSTM is similar to stacked LSTMs with an extra memory cell in the vertical dimension. In this thesis, two-dimensional Grid-LSTM will be used as a part of the architecture.
% Each of the above neural networks has its unique characteristics and has achieved tremen- dous success for large vocabulary continuous speech recognition tasks compared to con- ventional machine learning methods. Recently, research realized the advantage of the complementarity of CNNS, LSTMs, and DNNs can make up for their modeling limita- tions by combining variants of different neural networks into one unified architecture, like LSTMs followed by DNNs (LDNN), convolutional LDNN (CLDNN) and Grid LSTM followed by LDNN (GLDNN). These proposed architectures are designed to improve the capabilities of the RNNs. Pascanus and Bengio [40] proposed a deep transition RNNs and proved that the temporal modeling of RNNs could be improved if convert original input feature from lower level to higher level. Because the potential factors of varia- tion within the input feature can be refined and make it easier to modeling. Although some people believe stacked RNNs is already deep, there are some other aspects of the model might be regarded as shallow. For instance, the transition between two consec- utive hidden states at a same layer is shallow due to affine transformation followed by an element-wise nonlinearity. The implementations have already been applied like a few fully connected CNNs or Grid LSTMs.