\chapter{Summary and future works\label{cha:chapter6}}

% In this thesis, we propose three dierent kinds of methods to solve the task from ThyssenKrupp.
% The representation learning of signals generated by the rotation of BNA, a product made by
% ThyssenKrupp in Germany. The learned representation z is supposed to reveal the relationship
% between the quality of the nal product steering gear, which is known label data y 2 f0; 1g,
% and the quality of the BNA, which is unknown.
% The first method we propose is based on time delay embedding, Wasserstein distance and
% multidimensional scaling (MDS). The idea of time delay embedding comes from dynamical
% system analysis. One example of using this method is to reconstruct the attractor. Here we
% interpret the result from time delay embedding to probability distribution using delta function.
% Then we measure the difference between each probability distribution using Wasserstein
% distance and nd the representation of the desired dimension by MDS.
% Time delayed embedding method is purely based on time domain information. We also use
% real Fourier transform to get spectrogram from the time series. The measure is then dened
% based on the spectrogram. The rest of the steps are the same.
% We tried NSynth dataset at rst and then dataset from ThyssenKrupp. The experiment
% shows one certain area of learned representations, whose corresponding spectrograms have a
% large amplitude in the interval of high frequencies.
% The second method is variational autoencoder (VAE), which is based on advanced deep
% learning. We rstly used MNIST dataset to see the VAE's capability of nding representation
% in greatly reduced dimension. Then we also use VAE to generate new hand-written digits,
% which doesn't exist in the training or test dataset. After making sure that the implementation is
% correct, we used spectrogram of the signal as input to train our VAE. The learned representation
% from VAE shows certain consistency with the result from spectrogram. The consistency is that
% the learned representation of spectrogram with huge jumps is located at the tail of the point
% cloud, which means that both methods extract the same feature of the input data and represent
% them in the latent space dierently.
% The third method is the SMI method. It is a method based on information theory and
% applied in positive unlabel data situation. Positive unlabel data means that the training set
% and test set are composed of data with the positive label and unlabeled data. Unlabel data
% is a mixture of positive data and negative data. This scenario perfectly ts the dataset given
% by ThyssenKrupp. In our case, the positive data are generated by the good nal product. We
% are sure that every component of steering gear is good in this case. And so it is for BNA. The
% unlabel data are generated by the defect nal product since we don't know whether it is caused
% by the BNA or not. It is composed of positive data (good BNA) and negative data (defect
% BNA). SMI model rstly learns the representation and then classies the data.

% The experiments based on MNIST dataset show that the SMI model is able to learn the
% representation of the unlabel data in lower dimensional space eciently and classify whether
% the unlabel data is from positive data or from negative data. It also produces the prediction
% of the mixing coecient of the positive data among the unlabel data. The result of the SMI
% method using ThyssenKrupp data shows two clusters. The learned representations of defect
% nal products are mostly located at axis z2 = 0.
% We nd consistency in the results generated by these three methods. In experiments based
% on the rst two methods, the points of the spectrogram with high amplitude are located either
% at the periphery of the only cluster or at the tails of the two clusters. In the point cloud
% generated by the SMI model, those points are now mostly located horizontally.
% Because of the limited computation power, we didn't try many tuning techniques for the
% hyperparameters of the neural network. Further studies include trying dierent hyperparameters
% to check the performance of the proposed methods. Another possible approach is to add a
% classier to the VAE model and use the joint loss to train the encoder, decoder, and classier
% simultaneously. This might improve the performance of the representation learning process
% and also directly gives us the prediction about the quality of the BNA. In general, this task
% from ThyssenKrupp is a combination of time series representation learning and semi-supervised
% learning. Because only the given label data of good nal product provides information for the
% task. The label data of defect nal products cannot provide as much information as the label
% of the good ones. More semi-supervise learning techniques are another main focus in further
% studies.

The objective of this thesis is to approximate a wide class of hysteretic systems by neural networks. We propose a new network architecture, namely HNN, which is a realization of a linear combination of nonlinear plays. We compare the performance between HNN and LSTM networks, and it shows that HNN beats LSTM. Additionally, an illustrative application, financial market,  is generalized by introducing \textit{agents N}. We use both HNN and LSTM networks to learn from the synthetic data sets generated from the market model and compare the results between them. 
It reveals that HNN can learn the market model better than LSTM networks do.

Because of the limited computation power, we didn't exploit multiple tuning techniques for the
hyperparameters of the neural networks. Further studies include trying different hyperparameters to check the performance of the proposed methods. Another possible approach to learn the market model is using expectation-maximization algorithm instead of the direct learning algorithm given in \mysectionref{sec:chapter4:direct_learning}. By EM algorithm, we can also get a sequence $b_1, b_2, \ldots, b_N$ of most probable values of noise and proceed analogously to \mysectionref{sec:chapter2:training-pi-network}. Moreover, we can evaluate real-world data set, such as SP price sequence, and compare the performance of HNN with the state-of-the-art networks.

% In this chapter the implementation of Component X is evaluated. An example instance was created for every service. The following chapter validates the component implemented in the previous chapter against the requirements.
% \\
% \\
% Put some screenshots in this section! Map the requirements with your proposed solution. Compare it with related work. Why is your solution better than a concurrent approach from another organization?

% \section{Test Environment\label{sec:testenvir}}

% Fraunhofer Institute FOKUS' Open IMS Playground was used as a test environment for the telecommunication services. The IMS Playground ...

% \section{Scalability\label{sec:scal}}

% Lorem Ipsum

% \section{Usability\label{sec:usab}}

% Lorem Ipsum

% \section{Performance Measurements\label{sec:performance}}

% Lorem Ipsum
