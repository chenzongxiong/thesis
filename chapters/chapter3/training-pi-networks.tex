% \subsection{Training a PI network\label{sec:chapter3:training-pi-network}}
% Assume we are given an input sequence $x_1, x_2, \ldots, x_N$ and an output sequence $q_1, q_2, \ldots, q_N$. We perform the following steps in cycle until convergence.

% \begin{enumerate}
% \item Preparing initial states for the $m$-unfolded network: Fix a vector of initial states $P_0$ and all the weights (denoted by $W$). For each $k=1, \ldots, K$, we calculate recursively $p_{1}^{k}, p_{2}^{k}, \ldots, p_{N}^{k}$ by formula \ref{eqn:chapter3:play-operator}. We denote the corresponding (intermediate) states of the \textbf{PI} operator by
%   \begin{equation*}
%     P_n = (p_{n}^{1}, \ldots, p_{n}^{K}), n = 1, \ldots, N.
%   \end{equation*}
% \item Preparing inputs for the $m$-unfolded network: We fix $m$ and group the input sequence into $m$-tuples:
%   \begin{equation*}
%     \mathbf{x_1} := (x_1, \ldots, x_m), \quad \mathbf{x_2} := (x_2, \ldots, x_{m+1}), \quad \ldots,
%   \end{equation*}
%   which gives $M := N-m$ tuples $\mathbf{x_1}, \ldots, \mathbf{x_M}$. Next we form a new set of inputs for the $m$-unfolded network, attaching the vectors of intermediate states:
%   \begin{equation*}
%     \mathbf{y_1} := (P_0, \mathbf{x_1}), \quad \mathbf{y_2} := (P_1, \mathbf{x_2}), \quad \ldots,
%   \end{equation*}

% \item Training the $m$-unfolded network: We train by stochastic gradient descent the feed-forward $m$-unfolded \textbf{PI} network
%   \begin{equation*}
%     \mathbb{R}^{K} \times \mathbb{R}^{m} \ni \mathbf{y} \mapsto F_{m}(\mathbf{y}) \in \mathbb{R}^m
%   \end{equation*}
%   with the inputs $\mathbf{y_1}, \ldots, \mathbf{y_M}$ and the true targets $\mathbf{q_1}, \ldots, \mathbf{q_M}$, where
%   \begin{equation}
%     \begin{aligned}
%       \mathbf{q_1} = (q_1, \ldots, q_m), \quad 
%       \mathbf{q_2} = (q_2,\ldots, q_{m+1}), \quad 
%       \ldots
%     \end{aligned}
%   \end{equation}

% % \item We update the initial state $P_0$:
% %   \begin{equation*}
% %     P_{0}^{new} := P_{0} - \nabla_{P_0} (F_m(P_0, \mathbf{x_1}) - \mathbf{q_1})^2
% %   \end{equation*}
% \item Updating weights $W$ and initial state $P_0$ 
%   \begin{eqnarray*}
%     W^{new} := W - \nabla_{W} (F_m(P_0, \mathbf{x_1}) - \mathbf{q_1})^2 \\
%     P_{0}^{new} := P_{0} - \nabla_{P_0}(F_m(P_0, \mathbf{x_1}) - \mathbf{q_1})^2
%   \end{eqnarray*}
% \item \mytodo{TODO: Training state}
% \item \todo{TODO: Training weights}

% \end{enumerate}

% \subsection{Analysis of $p_0$}
% For $k>0$
% \begin{equation}
%   \begin{aligned}
%     \Phi(x) =
%     \begin{cases}
%       k(x - \frac{1}{2}), & x \in [\frac{1}{2}, +\infty) \\
%       0,               &  x \in (-\frac{1}{2}, \frac{1}{2}) \\
%       k(x + \frac{1}{2}), & x \in (-\infty, \frac{1}{2}]
%     \end{cases}
%   \end{aligned}
% \end{equation}

% \begin{eqnarray*}
% p_n &=& \Phi(w x_{n} - p_{n-1})- p_{n-1} \\
% \frac{\partial p_{n}}{\partial p_{n-1}} &=& 1- \frac{\partial \Phi(u_n)}{\partial u_n}\Bigr|_{\substack{u_n=w x_{n} - p_{n-1}}} 
% \end{eqnarray*}

% \begin{equation}\label{eqn:chapter3:derviation-phi}
%   \begin{aligned}
%     \frac{\partial \Phi(u_i)}{\partial u_i} =
%     \begin{cases}
%       k, & p_{n-1} \in (-\infty, w x_{n}-\frac{1}{2}] \cup [w x_{n}+\frac{1}{2}, +\infty) \\
%       0, &  p_{n-1} \in (w x_{n}-\frac{1}{2}, w x_{n}+\frac{1}{2})
%     \end{cases}
%   \end{aligned}
% \end{equation}

% \begin{equation}\label{eqn:chapter3:dynamic-p0}
% \frac{\partial p_{n}}{\partial p_0}= \prod_{i=1}^{n} \frac{\partial p_{i}}{\partial p_{i-1}} = \prod_{i=1}^{n} \left(1- \frac{\partial \Phi(u_i)}{\partial u_i}\Bigr|_{\substack{u_i=w x_{i} - p_{i-1}}}\right)
% \end{equation}


% Combining \ref{eqn:chapter3:derviation-phi} and \ref{eqn:chapter3:dynamic-p0}, if $n$ is sufficiently large, we conclude that 
% \begin{enumerate}
%     \item $k \in (0, 2]$, gradient on $p_0$ vanishes
%     \item $k \in (2, +\infty)$, gradient on $p_0$ explodes.
% \end{enumerate}

% Solution: using batch training method may avoid gradient vanishing and exploding, however, the batch size cannot be too large. 