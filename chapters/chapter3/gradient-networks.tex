\subsection{Gradient of networks}\label{sec:chapter3:gradient-networks}
First we only consider \textbf{one play}
\begin{equation}\label{eqn:chapter3:outputs-of-pi-networks}
G(P_{n}, w^{1}) = \sum_{i=1}^{S} \tilde{\theta_{i}} \tanh(\theta_{i} P_{n} + \theta_{i0}) + \tilde{\theta_{0}}
\end{equation}

Where $P_{n} = [p_{1}, p_{2}, \ldots, p_{n}]$, $G(P_{n}, w^{1}) = [y_{1}, y_{2}, \ldots, y_{n}]$, $\forall{i} \in [1, ..., S]$, $\theta_{i} P_{n} = (\theta_{i} p_{1}, \theta_{i} p_{2}, \ldots, \theta_{i} p_{n})$,

So $\tanh(\theta_{i} P_{n} + \theta_{i0}) = [\tanh(\theta_{i} p_{1} + \theta_{i0}), \tanh(\theta_{i} p_{2} + \theta_{i0}), \ldots, \tanh(\theta_{i} p_{n} + \theta_{i0})]$

So $\sum_{i=1}^{S} \tilde{\theta_{i}} \tanh(\theta_{i} P_{n} + \theta_{i0}) + \tilde{\theta_{0}} = [\sum_{i=1}^{S} \tilde{\theta_{i}} \tanh(\theta_{i} p_{1} + \theta_{i0}) + \tilde{\theta_{0}},
\sum_{i=1}^{S} \tilde{\theta_{i}} \tanh(\theta_{i} p_{2} + \theta_{i0}) + \tilde{\theta_{0}},
\ldots,
\sum_{i=1}^{S} \tilde{\theta_{i}} \tanh(\theta_{i} p_{n} + \theta_{i0}) + \tilde{\theta_{0}}] =
[y_{1}, y_{2}, ..., y_{n}]$.

Take $y_{j}$, where $j \in [1, ..., n]$ for example

Let $z_j=\theta_i p_j + \theta_{i0}$ and $f(z_j) = \tanh(\theta_i p_j + \theta_{i0})$, we obtain
\begin{equation}\label{eqn:chapter3:TODO}
y_{j}  = \sum_{i=1}^{S} \tilde{\theta_{i}} \tanh(\theta_{i} p_{j} + \theta_{i0}) + \tilde{\theta_{0}}  \\
       = \sum_{i=1}^{S} \tilde{\theta_{i}} f(z_j) + \tilde{\theta_{i0}}
\end{equation}

Calculate derivation for $y_{j}$,
\begin{equation}\label{eqn:chapter3:TODO}
\frac{\partial y_{j}}{\partial p_{j}} = \sum_{i=1}^{S} \tilde{\theta_{i}} \theta_{i} \frac{\partial f(z_j)}{\partial z_{j}} \frac{\partial z_{j}}{\partial p_{j}}
\end{equation}

Now let's consider the mapping between $p_{j}$ and $x_{j}$. let $\sigma_{j} = w^{1} x_{j} - p_{j-1}$
\begin{equation}\label{eqn:chapter3:TODO}
p_{j} = \Phi(\sigma_{j}) + p_{j-1}
\end{equation}

and

\begin{equation}\label{eqn:chapter3:TODO}
\Phi(x) =
        \begin{cases}
        x - 1/2, & x > 1/2 \\
        0, & -1/2 < x < -1/2 \\
        x + 1/2, & x < -1/2 \\
        \end{cases}
\end{equation}

Using chain rule, we obtain
\begin{equation}
\frac{\partial y_{j}}{\partial x_{j}} = \frac{\partial y_{j}}{\partial p_{j}} \frac{\partial p_{j}}{\partial x_{j}} \\
                                      = \sum_{i=1}^{S} \tilde{\theta_{i}} \theta_{i} w^{1} \frac{\partial f(z_j)}{\partial z_{j}} \frac{\partial{\Phi(\sigma_{j})}}{\partial{\sigma_{j}}}
\end{equation}


To consider \textbf{multiple plays} case, we reformulate the derivation as following:

\begin{equation}
\frac{\partial {y_{j}^{1}}}{\partial x_{j}} = \frac{\partial{y_{j}^{1}}}{\partial{p_{j}^{1}}} \frac{\partial{ p_{j}}^{1}}{\\partial x_{j}} \\
                                      = \sum_{i=1}^{S} \tilde{\theta_{i}^{1}} \theta_{i}^{1} w^{1} \frac{\partial f(z_{j}^{1})}{\partial z_{j}^{1}} \frac{\partial{\Phi(\sigma_{j}^{1})}}{\partial{\sigma_{j}^{1}}}
\end{equation}


Now from the architecture, we know that if we have $P$ plays,
\begin{equation}
F = \frac{1}{P} \sum_{k=1}^{P} G^{k}
\end{equation}
Where \(F=[f_1, f_2, ..., f_n]\),
and
\begin{equation}
f_{j} = \frac{1}{P} \sum_{k=1}^{P} y_{j}^{k}
\end{equation}

our derivation is:

\begin{equation}
\frac{\partial f_{j}}{\partial x_{j}} = \frac{1}{P} \sum_{k=1}^{P} \frac{\partial {{y_{j}^{k}}}}{\partial {{x_{j}}}} \\
               = \frac{1}{P} \sum_{k=1}^{P} \frac{\partial {y_{j}^{k}}}{\partial {p_{j}^{k}}} \frac{\partial {p_{j}^{k}}}{\partial {x_{j}}} \\
               = \frac{1}{P} \sum_{k=1}^{P}  \sum_{i=1}^{S} \tilde{\theta_{i}^{k}} \theta_{i}^{k} w^{k} \frac{\partial f(z_{j}^{k})}{\partial z_{j}^{k}} \frac{\partial{\Phi(\sigma_{j}^{k})}}{\partial{\sigma_{j}^{k}}}
\end{equation}

