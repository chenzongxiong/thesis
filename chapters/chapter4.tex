


\chapter{Hyteretic neural networks for financial market model\label{cha:chapter4}}
In this chapter, we discuss how to learn our market model proposed in \mychapterref{cha:chapter3} by HNN. We also provide a way to forecast the future price by HNN.


\section{Assumptions}
\label{sec:chapter4:assumption}
\myupdate{Assume that} we observe a sequence of price $x_1, x_2, ..., x_N$ for a fixed $N > 0$. Based on \myassumptionref{assumption:chapter3:demand/supply-price-formation} in \myupdate{\mychapterref{cha:chapter3}}, we assume that the price $x_{n}$ hysteretically depends on the underlying noise $b_n$, with $b_n$ being a random walk \myassumptionref{assumption:chapter3:b_sequence}, what we described in the previous chapter. Using the notation

\begin{equation}\label{eqn:chapter4:define_b_p}
   \mathcal{B}_n := (b_1, b_2, \ldots, b_n), \quad \mathcal{X}_n := (x_1, x_2, \ldots, x_n)
\end{equation}

% we have
\begin{equation}\label{eqn:chapter4:random-walk}
b_{0} = 0, \quad b_{n} \thicksim \mathcal{N} (b_{n-1} + \mu, \sigma)
\end{equation}


The underlying noise $b_n$ can be expressed as a hysteresis operator depending on the observed price $x_n$ (\myassumptionref{assumption:chapter3:transitions-between-stabilized-prices}), i.e., 
\begin{equation}\label{eqn:chapter4:g-network}
b_n=G(\mathcal{X}_n, W)
\end{equation}
where $G$ is a hysteresis operator parameterized by a vector $W$. The vector $W$ contains the weights of the whole network.


\section{Learning \myupdate{the financial market model}}\label{sec:chapter4:direct_learning}
We \myupdate{described} an approach to learn HNN in case of data sets given ground-truth outputs in \mysectionref{sec:chapter2:training-pi-network}. However, we can not apply this approach straightforwardly in the financial market model proposed in \mychapterref{cha:chapter3} because we cannot observe the sequence of random walk $\mathcal{B}_N$ in the real-world market directly. Instead,
we learn the parameters $W$, $\mu$ and $\sigma$ of the network $G$ by maximizing
the likelihood of $\mathcal{X}_N$ (see \myformularef{eqn:chapter4:define_b_p}). Since $\mathcal{X}_N$ is the deterministic function of a random variable
$\mathcal{B}_N$ (see \myformularef{eqn:chapter4:define_b_p}), its probability density is given by
\begin{equation}\label{eqn:chapter4:pP}
\begin{aligned}
p(\mathcal{X}_N) &= p(x_1, x_2, ..., x_N) \\
               &= p_b(b_1, b_2, ..., b_N) \left|\det \mathcal{J(X_\textit{N})}\right| \\
               &= p_b(G(\mathcal{X}_1, W), G(\mathcal{X}_2, W), \ldots, G(\mathcal{X}_N, W)) \left|\det \mathcal{J(X_\textit{N})}\right|
\end{aligned}
\end{equation}

where
\begin{equation}\label{eqn:chapter4:pB}
\begin{aligned}
p_b({\mathcal{B}_N}) &= p_b(b_1, b_2, ..., b_N) \\
                   &= \prod_{n=1}^{N} p_b(b_n|b_{n-1}) \\
                   &= \prod_{n=1}^{N} \frac{1}{\sqrt{2 \pi} \sigma} \exp\left(-\frac{(b_{n}-b_{n-1}-\mu)^2}{2 \sigma^2}\right)
\end{aligned}
\end{equation}
is the probability distribution of $\mathcal{B}_N$ and $\mathcal{J(P_\textit{N})}$ is the Jacobian matrix. Recall that \(G\) has the
causality property \citep[p. 10]{krejci1996hysteresis}, hence $\mathcal{J(P_\textit{N})}$ is a triangular matrix
\begin{equation}
J(\mathcal{X_\textit{N}}) = 
\Scale[1.3]{
\begin{bmatrix}\label{eqn:chapter4:Jp}
    \frac{\partial{b_1}}{\partial{x_1}} & 0 & 0 & \dots  & 0 \\
    \frac{\partial{b_2}}{\partial{x_1}} & \frac{\partial{b_2}}{\partial{x_2}} & 0 & \dots  & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial{b_N}}{\partial{x_1}} & \frac{\partial{b_N}}{\partial{x_2}} & \frac{\partial{b_N}}{\partial{x_3}} & \dots  & \frac{\partial{b_N}}{\partial{x_N}}
\end{bmatrix}
}
\end{equation}

where $b_j = G(\mathcal{X}_j, W), \, j = 1, \ldots, N$ (see \myformularef{eqn:chapter4:g-network}).

% \noindent
Therefore, \myformularef{eqn:chapter4:pP}, \myformularef{eqn:chapter4:Jp} and \myformularef{eqn:chapter4:pB} yield
\begin{equation}
\begin{aligned}
p(\mathcal{X}_N) &= p_b(G(\mathcal{X}_1, W), G(\mathcal{X}_2, W), \ldots, G(\mathcal{X}_N, W)) \left|\det \mathcal{J(X_\textit{N})}\right| \\
               &= \prod_{n=1}^{N} \frac{1}{\sqrt{2 \pi} \sigma} \exp\left(-\frac{(b_{n}-b_{n-1}-\mu)^2}{2 \sigma^2}\right)
                   \prod_{n=1}^{N} \left| \frac{\partial b_n}{\partial x_n} \right| \\
               &= \prod_{n=1}^{N} \frac{1}{\sqrt{2 \pi} \sigma} \exp\left(-\frac{(b_{n}-b_{n-1}-\mu)^2}{2 \sigma^2}\right) \left|\frac{\partial b_n}{\partial x_n}\right|
\end{aligned}
\end{equation}

Thus, maximizing the log-likelihood of \(p(\mathcal{X}_N)\) is equivalent to minimize negative log-likelihood $\mathcal{L}_{ML}$ as follows:
\begin{equation}
\begin{aligned}
\mathcal{L}_{ML} = -\ln p(\mathcal{X}_N) &= \sum_{n=1}^{N} \left(\frac{(b_n-b_{n-1}-\mu)^2}{2 \sigma^2} + \frac{1}{2} \ln (2 \pi \sigma^2) - \ln \left|\frac{\partial b_n}{\partial x_n}\right|\right) \\ 
&\thicksim \sum_{n=1}^{N} \left(\frac{(b_n-b_{n-1}-\mu)^2}{\sigma^2} + \ln \sigma^2 - \ln \left(\frac{\partial b_n}{\partial x_n}\right)^2 \right) \\
& \longrightarrow{\min_{W, \mu, \sigma}} 
\end{aligned}
\end{equation}

As a by-product, we obtain a \myupdate{most likely} sequence \myupdate{$b_j = G(\mathcal{X}_j, W), j = 1, \ldots, N$} of the noise levels. 

\begin{remark}\label{remark:chapter4:standard-deviation}
The standard deviation $\sigma$ need not be learned as it encodes only the units of the stocks.
\end{remark}

\section{Predicting}\label{sec:chapter4:predicting_price}
For the method described in \mysectionref{sec:chapter4:direct_learning}, we predict the \myupdate{distribution of the} next price $x^{predicted}_{N+1}$ as follows: 
\begin{enumerate}
    \item Sampling the next random walk $b_{N+1} \sim \mathcal{N}(b_{N} + \mu, \sigma)$. 
    \item Determining the direction of price trend: If $b_{N+1} - b_{N} < 0$, we know that external agents buy stocks from the market (see \mychapterref{cha:chapter3}) and the price should rise. \mydelete{In verse,}\myupdate{If} $b_{N+1} - b_{N} > 0$, the price should drop.
    \item Brute force search the next price $\hat{p}_{N+1}$ 
    : At moment $N$, we already observed price $x_N$. We can set next price $\hat{x}_{N+1} = x_{N} + \Delta$ and iterate all possible $\Delta$ until the condition $b_{N+1} = G(\hat{x}_{N+1}, W)$ \myupdate{is} satisfied. 
    \item Repeating from step $1$ to step $3$ multiple times, \myupdate{we obtain a histogram approximating the distribution of the next price $x_{N+1}$}
    \item Calculating the average of the predicted next price $x^{predicted}_{N+1}$ or its distribution (see \myremarkref{remark:chapter4:predicting})
\end{enumerate}

Once we observed the true price $x_{N+1}$, we \mydelete{predict}\myupdate{reconstruct} the next noise level: 
\begin{equation}
    b_{N+1} = G(\mathcal{X}_{N+1}, W)
\end{equation}
We keep predicting the next price and the next noise level in turn.

\begin{remark}\label{remark:chapter4:predicting}
Using the average of next predicted prices in step 5 during predicting is not always good in real-world scenarios, especially if the distribution of prices has several peaks (see \myfigureref{fig:chapter5:predicting-price-157-price} and \myfigureref{fig:chapter5:predicting-price-158-price}). We can make a more sophisticated trading decision according to the distribution of prices obtained from step 4, compared with standard models, i.e., \citep{daniel2019financial}, which predict the next price based on the observed prehistory.
\end{remark}

