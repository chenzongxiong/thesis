\chapter{HNN for Market\label{cha:chapter4}}

In this chapter, we combine the market model and HNN together, and using HNN to train our market model and do prediction.

\section{Assumptions}
\label{sec:chapter4:assumption}
Assume, we observe prices $p_1, p_2, ..., p_N$ for a fixed $(N > 0$. Based on \citep{dima2014},
assume that the price $p_{n}$ hysteretically depends on the underlying noise $b_n$, with $b_n$ being a Brownian motion (random walk). Using the notation

\begin{equation*}
   \mathcal{B}_n := (b_1, b_2, \ldots, b_n), \quad \mathcal{P}_n := (p_1, p_2, \ldots, p_n)
\end{equation*}

we have
\begin{equation}
b_{0} = 0, \quad b_{n} \thicksim \mathcal{N} (b_{n-1} + \mu_{b}, \sigma_{b})
\end{equation}
\begin{equation}
p_{n} = F(\mathcal{B}_n, W_{p})
\end{equation}

where $F$ is a hysteresis operator parametrized by a vector $W_p$. The vector $W_p$ contains the weights of the network.


Based on \citep{dima2014} again, the underlying noise $b_n$ can be expressed as a hysteresis operator depending on the observed prices $p_n$, i.e.,

\begin{equation}
b_n=G(\mathcal{P}_n, W_b)
\end{equation}

where $G$ is a hysteresis operator parametrized by a vector $W_b$. The vector $W_b$ contains the weights of the network.

If $G$ network is PI operators, there exists $F$ network consisted of PI operators, holding
\begin{equation}
   F \approx G^{-1}
\end{equation}

% Even through one explicitly adds $N$'s trading strategy in \mychapterref{cha:chapter3}, $\mathcal{G}$ becomes Preisach and $\mathcal{F}$ is not Preisach anymore.

\section{Direct Learning}
\label{sec:chapter4:direct_learning}
We learn the parameters $W_b, \mu_b, \sigma_b$ of the network $G$ by maximizing
the likelihood of $\mathcal{P}$. Since $\mathcal{P}$ is the determenistic function of a random variable
$\mathcal{B}$, its probability density is given by
\begin{equation}\label{eqn:chapter4:pP}
\begin{aligned}
p(\mathcal{P}) &= p(p_1, p_2, ..., p_N) \\
               &= p_b(b_1, b_2, ..., b_N) \left|\det \mathcal{J(P)}\right| \\
               &= p_b(G(\mathcal{P}_1, W_b), G(\mathcal{P}_2, W_b), \ldots, G(\mathcal{P}_N, W_b)) \left|\det \mathcal{J(P)}\right|
\end{aligned}
\end{equation}

where
\begin{equation}\label{eqn:chapter4:pB}
\begin{aligned}
p_b({\mathcal{B}}) &= p_b(b_1, b_2, ..., b_N) \\
                   &= \prod_{n=1}^{N} p_b(b_n|b_{n-1}) \\
                   &= \prod_{n=1}^{N} \frac{1}{\sqrt{2 \pi} \sigma} \exp\left(-\frac{(b_{n}-b_{n-1}-\mu_{b})^2}{2 \sigma^2}\right)
\end{aligned}
\end{equation}
is the probability distribution of $\mathcal{B}$ and $\mathcal{J(P)}$ is the Jacobian matrix. Recall that \(G\) has the
causality property \lackref{}, hence $\mathcal{J(P)}$ is a triangular matrix
\begin{equation}
J(\mathcal{P}) = 
\Scale[1.3]{
\begin{bmatrix}\label{eqn:chapter4:Jp}
    \frac{\partial{b_1}}{\partial{p_1}} & 0 & 0 & \dots  & 0 \\
    \frac{\partial{b_2}}{\partial{p_1}} & \frac{\partial{b_2}}{\partial{p_2}} & 0 & \dots  & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial{b_n}}{\partial{p_1}} & \frac{\partial{b_n}}{\partial{p_2}} & \frac{\partial{b_n}}{\partial{p_3}} & \dots  & \frac{\partial{b_n}}{\partial{p_n}}
\end{bmatrix}
}
\end{equation}

Therefore, \myformularef{eqn:chapter4:pP}, \myformularef{eqn:chapter4:Jp} and \myformularef{eqn:chapter4:pB} yield
\begin{equation}
\begin{aligned}
p(\mathcal{P}) &= p_b(G(\mathcal{P}_1, W_b), G(\mathcal{P}_2, W_b), \ldots, G(\mathcal{P}_N, W_b)) \left|\det \mathcal{J(P)}\right| \\
               &= \prod_{n=1}^{N} \frac{1}{\sqrt{2 \pi} \sigma} \exp\left(-\frac{(b_{n}-b_{n-1}-\mu_{b})^2}{2 \sigma^2}\right)
                   \prod_{n=1}^{N} \left| \frac{\partial b_n}{\partial p_n} \right| \\
               &= \prod_{n=1}^{N} \frac{1}{\sqrt{2 \pi} \sigma} \exp\left(-\frac{(b_{n}-b_{n-1}-\mu_{b})^2}{2 \sigma^2}\right) \left|\frac{\partial b_n}{\partial p_n}\right|
\end{aligned}
\end{equation}

Thus, maximizing the log-likelihood of \(p(\mathcal{P})\) is equivalent to minimize negative log-likelihood $L$ as following:
\begin{equation}
\begin{aligned}
L = -\ln p(\mathcal{P}) &\thicksim \sum_{n=1}^{N} \left(\frac{(b_n-b_{n-1}-\mu_{b})^2}{2 \sigma_{b}^2} + \ln \sigma_{b} - \ln \left|\frac{\partial b_n}{\partial p_n}\right|\right) \\
& \longrightarrow{\min_{W_b, \mu_b, \sigma_b}} 
%   &= \frac{1}{2} \sum_{n=1}^{N} \left[\left(\frac{b_n - b_{n-1} - \mu_{b}}{\sigma_b} \right)^2 + 2 \ln \sigma_{b} - 2 \ln \left|\frac{\partial b_n}{\partial p_n}\right|\right]
\end{aligned}
\end{equation}

As a by-product, we obtain a determenistic sequence $b_1,\ldots,b_N$ of the noise levels, which means that $p(\mathcal{B}|\mathcal{P})$ is a delta-distribution.
% \begin{equation}
% \begin{aligned}
% L = \ln p(\mathcal{P}) &\thicksim \sum_{n=1}^{N} \left(- \frac{(b_n-b_{n-1}-\mu_{b})^2}{2 \sigma_{b}^2} - \ln \sigma_{b} + \ln \left|\frac{\partial b_n}{\partial p_n}\right|\right) \\
%   &= - \frac{1}{2} \sum_{n=1}^{N} \left[\left(\frac{b_n - b_{n-1} - \mu_{b}}{\sigma_b} \right)^2 + 2 \ln \sigma_{b} - 2 \ln \left|\frac{\partial b_n}{\partial p_n}\right|\right]
% \end{aligned}
% \end{equation}
% It's also equivalent to minimize the loss function as following:
% \begin{equation}
% \begin{aligned}
% \min_{W_b, p_0} L &=& \min_{W_b, p_0} \sum_{n=1}^{N} \left[\left(\frac{b_n - b_{n-1} - \mu_{b}}{\sigma_b} \right)^2 - 2 \ln \left|\frac{\partial b_n}{\partial p_n}\right|\right] \\
%       &=& \min_{W_b, p_0} \sum_{n=1}^{N} \left[\left( b_n - b_{n-1} - \mu_{b}\right)^2 - 2 \sigma_{b}^2 \ln \left|\frac{\partial b_n}{\partial p_n}\right|\right]
% \end{aligned}
% \end{equation}

% From the target loss function, the only parameters we need to known is \(\mu_b\).

% Consider second part of loss function, \(\ln \frac{\partial b_{n}}{\partial p_{n}}\), we reformulate it to \(\ln \frac{\partial f_{j}}{\partial x_{j}} = \ln h_{j}\)

% \begin{eqnarray}
% \frac{ \partial \ln h_j}{\partial w^{k}} &=& \frac{1}{h_j} * \frac{\partial h_j} { \partial w^k}
% \end{eqnarray}

% \section{\mytodo{MSE and MLE}}
% \label{sec:org69ccd66}

% \begin{eqnarray}
% y &\thicksim& \mathcal{N} \left( y | G(x, w), \sigma^2 \right)
% \end{eqnarray}

% \begin{eqnarray}
% p(y) &=& \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(y-G(x, w))^2}{\sigma^2} \right)
% \end{eqnarray}

% \begin{eqnarray}
% p(\mathcal{Y}) &=& p (y_1, y_2, ..., y_N) \\
%               &=& \prod_{i}^N p(y_i)
% \end{eqnarray}

% \begin{eqnarray}
% \ln p(\mathcal{Y}) &=& \sum_{i} \left( -\frac{1}{2}\ln 2 \pi - \ln \sigma - \frac{(y_i - G(x_i, w)^2}{\sigma^2} \right)
% \end{eqnarray}

% Assume \(z_i = f(y_i)\), we obtain
% \begin{eqnarray}
% p(z_i) = p(f(y_i)) \left(\frac{\partial f}{\partial y_i}\right)^{-1} f^{-1}(z_i)
% \end{eqnarray}
% \(\max (\mu, \sigma, w)\) s.t. \(p(z_i)\) is maximal



% \section{Gradient of networks}\label{sec:chapter4:gradient-networks}
% First we only consider \textbf{one} generalized play
% \begin{equation}\label{eqn:chapter4:outputs-of-pi-networks}
% G(P_{n}, w^{1}) = \sum_{i=1}^{S} \tilde{\theta_{i}} \tanh(\theta_{i} P_{n} + \theta_{i0}) + \tilde{\theta_{0}}
% \end{equation}

% where $P_{n} = [p_{1}, p_{2}, \ldots, p_{n}]$, $G(P_{n}, w^{1}) = [y_{1}, y_{2}, \ldots, y_{n}]$, $\forall{i} \in [1, ..., S]$, $\theta_{i} P_{n} = (\theta_{i} p_{1}, \theta_{i} p_{2}, \ldots, \theta_{i} p_{n})$,

% So $\tanh(\theta_{i} P_{n} + \theta_{i0}) = [\tanh(\theta_{i} p_{1} + \theta_{i0}), \tanh(\theta_{i} p_{2} + \theta_{i0}), \ldots, \tanh(\theta_{i} p_{n} + \theta_{i0})]$

% So $\sum_{i=1}^{S} \tilde{\theta_{i}} \tanh(\theta_{i} P_{n} + \theta_{i0}) + \tilde{\theta_{0}} = [\sum_{i=1}^{S} \tilde{\theta_{i}} \tanh(\theta_{i} p_{1} + \theta_{i0}) + \tilde{\theta_{0}},
% \sum_{i=1}^{S} \tilde{\theta_{i}} \tanh(\theta_{i} p_{2} + \theta_{i0}) + \tilde{\theta_{0}},
% \ldots,
% \sum_{i=1}^{S} \tilde{\theta_{i}} \tanh(\theta_{i} p_{n} + \theta_{i0}) + \tilde{\theta_{0}}] =
% [y_{1}, y_{2}, ..., y_{n}]$.

% Take $y_{j}$, where $j \in [1, ..., n]$ for example

% Let $z_j=\theta_i p_j + \theta_{i0}$ and $f(z_j) = \tanh(\theta_i p_j + \theta_{i0})$, we obtain
% \begin{equation}\label{eqn:chapter4:TODO}
% y_{j}  = \sum_{i=1}^{S} \tilde{\theta_{i}} \tanh(\theta_{i} p_{j} + \theta_{i0}) + \tilde{\theta_{0}}  \\
%       = \sum_{i=1}^{S} \tilde{\theta_{i}} f(z_j) + \tilde{\theta_{i0}}
% \end{equation}

% Calculate derivation for $y_{j}$,
% \begin{equation}\label{eqn:chapter4:TODO}
% \frac{\partial y_{j}}{\partial p_{j}} = \sum_{i=1}^{S} \tilde{\theta_{i}} \theta_{i} \frac{\partial f(z_j)}{\partial z_{j}} \frac{\partial z_{j}}{\partial p_{j}}
% \end{equation}

% Now let's consider the mapping between $p_{j}$ and $x_{j}$. let $\sigma_{j} = w^{1} x_{j} - p_{j-1}$
% \begin{equation}\label{eqn:chapter4:TODO}
% p_{j} = \Phi(\sigma_{j}) + p_{j-1}
% \end{equation}

% and

% \begin{equation}\label{eqn:chapter4:TODO}
% \Phi(x) =
%         \begin{cases}
%         x - 0.5, & x > 0.5 \\
%         0, & -0.5 \le x \le 0.5 \\
%         x + 0.5, & x < -0.5 \\
%         \end{cases}
% \end{equation}

% Using chain rule, we obtain
% \begin{equation}
% \frac{\partial y_{j}}{\partial x_{j}} = \frac{\partial y_{j}}{\partial p_{j}} \frac{\partial p_{j}}{\partial x_{j}} \\
%                                       = \sum_{i=1}^{S} \tilde{\theta_{i}} \theta_{i} w^{1} \frac{\partial f(z_j)}{\partial z_{j}} \frac{\partial{\Phi(\sigma_{j})}}{\partial{\sigma_{j}}}
% \end{equation}


% To consider \textbf{multiple} generalized plays case, we reformulate the derivation as following:

% \begin{equation}
% \frac{\partial {y_{j}^{1}}}{\partial x_{j}} = \frac{\partial{y_{j}^{1}}}{\partial{p_{j}^{1}}} \frac{\partial{ p_{j}}^{1}}{\\partial x_{j}} \\
%                                       = \sum_{i=1}^{S} \tilde{\theta_{i}^{1}} \theta_{i}^{1} w^{1} \frac{\partial f(z_{j}^{1})}{\partial z_{j}^{1}} \frac{\partial{\Phi(\sigma_{j}^{1})}}{\partial{\sigma_{j}^{1}}}
% \end{equation}


% Now from the architecture, we know that if we have $P$ plays,
% \begin{equation}
% F = \frac{1}{P} \sum_{k=1}^{P} G^{k}
% \end{equation}
% Where \(F=[f_1, f_2, ..., f_n]\),
% and
% \begin{equation}
% f_{j} = \frac{1}{P} \sum_{k=1}^{P} y_{j}^{k}
% \end{equation}

% our derivation is:

% \begin{equation}
% \frac{\partial f_{j}}{\partial x_{j}} = \frac{1}{P} \sum_{k=1}^{P} \frac{\partial {{y_{j}^{k}}}}{\partial {{x_{j}}}} \\
%               = \frac{1}{P} \sum_{k=1}^{P} \frac{\partial {y_{j}^{k}}}{\partial {p_{j}^{k}}} \frac{\partial {p_{j}^{k}}}{\partial {x_{j}}} \\
%               = \frac{1}{P} \sum_{k=1}^{P}  \sum_{i=1}^{S} \tilde{\theta_{i}^{k}} \theta_{i}^{k} w^{k} \frac{\partial f(z_{j}^{k})}{\partial z_{j}^{k}} \frac{\partial{\Phi(\sigma_{j}^{k})}}{\partial{\sigma_{j}^{k}}}
% \end{equation}

% \section{\mytodo{analysis $\sigma$ is corresponded to scale only, it won't affect our final predictions}}

\section{Predicting}
For the method described in \ref{sec:chapter4:direct_learning}, we predict the next price $p^{predicted}_{N+1}$ as following: 
\begin{enumerate}
    \item Sampling the next random walk $b_{N+1} = b_{N} + \mathcal{N}(\mu_b, \sigma_b)$. 
    \item Determining the direction of price trend: If $b_{N+1} - b_{N} < 0$, we know that external agents buy stocks from the market (see \mychapterref{cha:chapter3}) and the price should rise. In verse, $b_{N+1} - b_{N} > 0$, the price should drop.
    \item Brute force search the next price $\hat{p}_{N+1}$ 
    : At moment $N$, we already observed price $p_N$. We can set next price $\hat{p}_{N+1} = p_{N} + \Delta$ and check whether the condition $b_{N+1} = G(\hat{p}_{N+1}, W_b)$ satisfied.
    \item Repeating from step $1$ to step $3$ multiple times
    \item Calculating the average the predicted next price $p^{predicted}_{N+1}$
\end{enumerate}

Once we observed the true price $p_{N+1}$, we predict the next noise level: 
\begin{equation}
    b_{N+1} = G(\mathcal{P}_{N+1}, W_b)
\end{equation}
We keep predicting the next price and the next noise level in turn.